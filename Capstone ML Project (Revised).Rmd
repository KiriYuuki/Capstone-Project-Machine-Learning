---
title: "Concrete Analysis"
author: "Yuanda Krisna"
date: "2024-08-12"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This problem was originally proposed by Prof. I-Cheng Yeh, Department of Information Management Chung-Hua University, Hsin Chu, Taiwan in 2007. It is related to [his research](https://www.researchgate.net/publication/222447231_Modeling_of_Strength_of_High-Performance_Concrete_Using_Artificial_Neural_Networks_Cement_and_Concrete_research_2812_1797-1808) in 1998 about how to predict compression strength in a concrete structure.

" Concrete is the most important material in civil engineering " - Prof. I-Cheng Yeh

Concrete compression strength is determined not just only by water-cement mixture but also by other ingredients, and how we treat the mixture. Using this dataset, we are going to find “the perfect recipe” to predict the concrete’s compression strength, and how to explain the relationship between the ingredients concentration and the age of testing to the compression strength.

Concrete: “Can you show me your recipe?”

How much the increment/decrement of the structure’s compression strength when you add more water? Can the concrete structure have more compression strength when you left it to rest longer?

The goal is to **build a linear regression model that fulfills all assumptions. Interpret how each ingredient and age of testing affect the concrete compression strength**.


```{r}
# Load necessary libraries
library(tidyverse)
library(caret)
library(car)  # For VIF calculation
```

# Data Preprocessing
```{r}
# Load the datasets
train <- read.csv("data/data-train.csv")
test <- read.csv("data/data-test.csv")

# Initial exploration
head(train)
summary(train)
glimpse(train)
```
This initial exploration is essential for understanding the structure and content of the dataset. Each function serves a different purpose:

1. head(train): Displays the first few rows of the dataset, giving a quick preview of the data and its format.

2. summary(train): Provides summary statistics for each variable in the dataset, such as minimum, maximum, mean, and quartiles. This helps in identifying the range and distribution of the data, as well as spotting any potential issues such as outliers or missing values.

3. glimpse(train): Offers a compact overview of the dataset, showing the type of each variable (e.g., numeric, character) and its content. This function is particularly useful for quickly assessing the data types and ensuring that they are appropriate for the analysis.

### Key Observations From the Initial Exploration:
1. Variable Distributions:

- Variables such as cement, slag, flyash, water, and strength have been summarized, and the results show their respective ranges, means, and quartiles.

- For example, cement has a mean value of 280.9 and ranges from 102.0 to 540.0 Kg/m³, indicating variability in the cement content across different concrete mixtures.

2. Age and Strength:

- The age variable, representing the number of days the concrete has been resting, ranges from 1 to 365 days, with a median of 28 days. This indicates that most concrete samples were tested within the first month.

- The strength variable, which is our target variable, ranges from 2.33 MPa to 82.60 MPa, with a median of 34.57 MPa. This wide range suggests that the dataset includes concrete with varying degrees of compressive strength.

3. Potential Issues:

- Skewness: Some variables, such as slag and super_plast, have minimum values of 0 and a large range, suggesting potential skewness. This might require transformation to ensure normality.

- Outliers: The summary also highlights the presence of extreme values in variables like strength, which could be outliers.


Next Steps:

Given the observations from this initial exploration, the next logical steps in the data preprocessing process would be:

1. Data Transformation and Scaling: 

Address potential skewness in variables like cement, slag, flyash, and others by applying appropriate transformations (e.g., square root, log). This step ensures that the data meets the assumptions of linear regression, particularly normality and homoscedasticity.

2. Outlier Detection and Handling: 

Investigate potential outliers in the dataset, particularly in key variables such as strength, water, and aggregate contents. Outliers can disproportionately influence model estimates, so it’s crucial to identify and address them before modeling.

## Data Transformation and Scaling
```{r}
# Apply square root transformation to reduce skewness
train_data <- train %>%
  mutate(cement = sqrt(cement),
         slag = sqrt(slag),
         flyash = sqrt(flyash),
         water = sqrt(water),
         super_plast = sqrt(super_plast),
         coarse_agg = sqrt(coarse_agg),
         fine_agg = sqrt(fine_agg))

# Apply standard scaling to the numeric variables
scaled_train_data <- train_data %>%
  select(-id, -strength) %>%  # Exclude id and strength
  scale()  # Standard scaling
```

The square root transformation is applied to the predictors to reduce skewness and help meet the linearity assumption of the linear regression model. Scaling is done to ensure that all numeric variables are on the same scale, preventing any feature from dominating the model due to its magnitude.

## Outlier Detection and Handling
```{r}
# Check for outliers in the target variable (strength) and important predictors
boxplot(train_data$strength, main="Boxplot of Strength")
boxplot(train_data$water, main="Boxplot of Water")
boxplot(train_data$coarse_agg, main="Boxplot of Coarse Aggregate")
boxplot(train_data$fine_agg, main="Boxplot of Fine Aggregate")
```

1. Boxplot of Strength

- The boxplot for the strength variable reveals a distribution of compressive strength values. The interquartile range (IQR) is represented by the box, and the whiskers extend to 1.5 times the IQR. Any data points beyond the whiskers are considered potential outliers. In this case, a few outliers are present at the higher end of the strength values, indicating some samples with unusually high compressive strength.

- Next Steps: 

These outliers in strength should be carefully considered. While they may represent natural variation in the data, they could also indicate errors or extreme cases that might disproportionately influence the model. We should consider whether to remove these outliers or keep them, depending on the overall context and our understanding of the data.

2. Boxplot of Water

- The boxplot for water shows that there are several outliers on both the lower and upper ends. These outliers represent concrete samples with unusually high or low water content, which could significantly affect the mixture's properties and, consequently, its compressive strength.

- Next Steps:

The presence of outliers in the water content is critical because water plays a crucial role in the hydration process of cement and directly impacts the concrete's strength. We may consider transforming the water variable or removing the extreme outliers, especially if they represent errors or very uncommon conditions.

3. Boxplot of Coarse Aggregate

- The boxplot for coarse_agg indicates a relatively symmetrical distribution with no apparent outliers. This suggests that the amount of coarse aggregate used across the samples is consistent and within a reasonable range, minimizing the need for intervention.

- Next Steps:

Since there are no outliers, we can proceed with this variable without any further transformations or outlier removal. This consistency in the coarse aggregate data suggests that it might be a stable predictor in the modeling process.

4. Boxplot of Fine Aggregate

- The boxplot for fine_agg shows a couple of outliers on both the lower and upper ends. These outliers might represent cases where the fine aggregate content deviates significantly from the norm, potentially affecting the workability and strength of the concrete.

- Next Steps:

Like with water, the presence of outliers in fine_agg needs to be addressed, as these extreme values could distort the relationship between fine aggregate and concrete strength. We could consider transformations or removing outliers if they are deemed to be errors or unrepresentative of typical conditions.

```{r}
# Identify outliers in strength and predictors
outliers_strength <- boxplot.stats(train$strength)$out
outliers_water <- boxplot.stats(train$water)$out
outliers_coarse_agg <- boxplot.stats(train$coarse_agg)$out
outliers_fine_agg <- boxplot.stats(train$fine_agg)$out

# Filter out the outliers
train_filtered <- train[!(train$strength %in% outliers_strength) &
                        !(train$water %in% outliers_water) &
                        !(train$coarse_agg %in% outliers_coarse_agg) &
                        !(train$fine_agg %in% outliers_fine_agg), ]

# Summary of the filtered data
summary(train_filtered)
```
In this step, outliers in the strength, water, coarse_agg, and fine_agg variables were identified using the boxplot.stats function, which detects data points lying beyond 1.5 times the interquartile range (IQR). These outliers were then filtered out from the dataset, resulting in a new dataset train_filtered with fewer observations.

Why Remove Outliers?

Outliers can have a disproportionate effect on statistical analyses, especially in regression models. They can skew the results, leading to inaccurate predictions and a model that does not generalize well to new data. By removing these outliers, we aim to create a more robust and reliable model that better represents the typical behavior of the data.

After removing outliers, it is essential to examine the summary statistics of the filtered dataset to understand how the data has changed:

1. Size of the Dataset: The number of observations has decreased from 825 to 784, reflecting the removal of 41 data points identified as outliers.

2. Central Tendencies and Distributions:

- The mean and median values for most variables, such as strength, water, cement, etc., have slightly shifted, indicating a more central distribution of data points after removing the extremes.

- The minimum and maximum values of the strength, water, and fine_agg variables have been adjusted downward, indicating the successful removal of extreme values that were identified as outliers.

3. Consistency Across Variables:

- The consistency and ranges of variables like coarse_agg and fine_agg are more representative of the overall dataset after removing the outliers. This consistency is crucial for building a model that can accurately capture the relationships between these variables and the target variable, strength.


# Exploratory Data Analysis

## Correlation Analysis
```{r}
# Calculate the correlation matrix for filtered data
correlation_matrix <- cor(train_filtered[, c("cement", "slag", "flyash", "water", "super_plast", "coarse_agg", "fine_agg", "age", "strength")])

# Print the correlation matrix
print(correlation_matrix)
```
### Key observations:

1. Cement and Strength (0.4928):

Cement shows a moderately strong positive correlation with compressive strength, indicating that as the cement content increases, the strength of the concrete tends to increase. This is expected because cement is the primary binding agent in concrete, contributing significantly to its compressive strength.

2. Superplasticizer and Strength (0.3603):

There is a moderate positive correlation between the use of superplasticizer and concrete strength. Superplasticizers are additives that increase the workability of the concrete mix while maintaining a lower water-to-cement ratio, which is beneficial for strength.

3. Age and Strength (0.3609):

Age has a moderate positive correlation with strength, suggesting that as the concrete ages, its compressive strength increases. This is consistent with the understanding that concrete continues to cure and gain strength over time.

4. Water and Strength (-0.3215):

Water has a negative correlation with strength, which is expected because higher water content in the mix can weaken the concrete by increasing porosity and reducing the overall strength. This relationship highlights the importance of controlling the water-to-cement ratio.

5. Fly Ash and Strength (-0.0791):

Fly ash shows a weak negative correlation with strength. This might suggest that in the current dataset, the inclusion of fly ash does not significantly contribute to compressive strength, or that its effects are overshadowed by other ingredients.

6. Coarse and Fine Aggregate:

Both coarse aggregate (-0.1354) and fine aggregate (-0.2099) have weak negative correlations with strength. These weak correlations suggest that while aggregates are essential for the concrete structure, their specific amounts may not have a strong direct impact on the compressive strength in this dataset.

7. Multicollinearity Concerns:

Superplasticizer and Water (-0.6677): There is a strong negative correlation between superplasticizer and water, which may suggest potential multicollinearity issues. This relationship needs to be monitored, as it could complicate the interpretation of the model coefficients.

#### Implications for Model Building:

1. Feature Selection:

- Cement, superplasticizer, and age are likely to be strong predictors of compressive strength and should be retained in the model.

- Water also plays a crucial role, but its negative correlation suggests that its effect needs to be carefully modeled, possibly with interaction terms or transformations.

- Fly ash and aggregates may require further exploration, but given their weak correlations, they might not contribute significantly to the predictive power of the model. However, removing them should be justified based on further analysis, such as variance inflation factor (VIF) checks for multicollinearity.

2. Model Complexity:

The moderate to strong correlations between some predictors (e.g., cement, superplasticizer) and strength suggest that a linear model could capture these relationships reasonably well. However, the potential multicollinearity between superplasticizer and water might necessitate additional modeling techniques, such as regularization (Ridge or Lasso regression) to stabilize coefficient estimates.

3. Interaction Terms:

The interplay between superplasticizer and water could be modeled with an interaction term to better capture their combined effect on concrete strength, addressing the high negative correlation between them.

## Scatter Plots for Key Relationship
```{r}
# Scatter plots for key predictors vs strength
plot(train$age, train$strength, xlab = "Age", ylab = "Strength", main = "Age vs. Strength")
plot(train$cement, train$strength, xlab = "Cement", ylab = "Strength", main = "Cement vs. Strength")
plot(train$super_plast, train$strength, xlab = "Superplasticizer", ylab = "Strength", main = "Superplasticizer vs. Strength")
plot(train$water, train$strength, xlab = "Water", ylab = "Strength", main = "Water vs. Strength")
```

1. Age vs. Strength

The scatter plot of Age vs. Strength shows a positive relationship, indicating that as the concrete ages, its compressive strength generally increases. This trend aligns with the typical behavior of concrete, which continues to cure and gain strength over time. However, the data points also reveal variability at different age levels, especially beyond 100 days, where the strength values start to spread more widely.

Implications:

- Positive Correlation: This positive correlation suggests that age is an important factor in predicting compressive strength.

- Non-linear Trends: The spread of data at higher ages might suggest a non-linear relationship, which could be explored further using polynomial terms or interaction effects in the model.

2. Cement vs. Strength

The scatter plot of Cement vs. Strength indicates a positive relationship, where higher amounts of cement generally lead to higher compressive strength. This relationship is consistent with the role of cement as the primary binding agent in concrete.

Implications:

- Strong Predictor: Cement appears to be a strong predictor of strength, as indicated by the upward trend in the scatter plot.

- Potential Non-linearity: While the overall trend is positive, the scatter plot also shows some variability, especially at higher cement levels. This could indicate the potential for non-linear effects, which might require further exploration in the modeling phase.

3. Superplasticizer vs. Strength

The scatter plot of Superplasticizer vs. Strength reveals a somewhat complex relationship. While there is a general positive trend, especially at higher levels of superplasticizer, the data points are quite scattered, particularly at lower levels.

Implications:

- Diminishing Returns: The scatter plot suggests that small amounts of superplasticizer might not have a consistent impact on strength, but higher amounts seem to contribute positively. This could be due to diminishing returns, where increasing the superplasticizer beyond a certain point yields less additional strength.

- Model Complexity: The variability in this relationship suggests that a simple linear model might not fully capture the effect of superplasticizer on strength. Non-linear modeling or interaction terms might be needed to better understand this relationship.

4. Water vs. Strength
The scatter plot of Water vs. Strength shows a negative relationship, where higher water content generally corresponds to lower compressive strength. This trend is expected, as excess water in the concrete mix can lead to higher porosity and reduced strength.

Implications:

- Negative Correlation: The negative correlation between water and strength underscores the importance of controlling the water-to-cement ratio in concrete mix design.

- Potential for Overfitting: The scatter plot shows significant variability, especially at intermediate water levels (around 180-200 Kg/m³). This variability might introduce challenges in modeling, potentially leading to overfitting if not carefully managed.

#### Overall Insight

1. Key Predictors:

- Cement and Age are clearly strong predictors of compressive strength, showing consistent positive relationships. These variables should be central to the predictive model.

- Superplasticizer and Water also play significant roles, but their relationships with strength are more complex, requiring careful modeling to capture their effects accurately.

2. Modeling Considerations:

- Given the potential for non-linearity in these relationships, especially with Superplasticizer and Water, it may be beneficial to explore polynomial terms or interaction effects in the model.

- The scatter plots also suggest that simple linear regression might not fully capture the relationships between these variables and strength. More sophisticated techniques, such as regularization or non-linear regression, might be needed.

3. Feature Engineering:

- Consider creating interaction terms, such as Cement x Water or Superplasticizer x Water, to better capture the combined effects of these variables on strength.

- Polynomial terms for Age might also be explored to account for the increasing variability in strength at higher age levels.

4. Multicollinearity Check:

Before moving to model fitting, it will be important to check for multicollinearity, particularly given the strong negative correlation between Water and Superplasticizer observed earlier.

## Distribution Analysis
```{r}
# Histogram for each variable
par(mfrow = c(3, 3))  # Set up a 3x3 grid for plots

hist(train$cement, main = "Cement")
hist(train$slag, main = "Slag")
hist(train$flyash, main = "Fly Ash")
hist(train$water, main = "Water")
hist(train$super_plast, main = "Superplasticizer")
hist(train$coarse_agg, main = "Coarse Aggregate")
hist(train$fine_agg, main = "Fine Aggregate")
hist(train$age, main = "Age")
hist(train$strength, main = "Strength")

par(mfrow = c(1, 1))  # Reset layout to default
```

### The histograms provide a visual representation of the distribution of each variable in the dataset.

1. Cement:

The distribution of cement amounts is slightly right-skewed, with the majority of data points concentrated between 100 and 400 Kg/m³. There are fewer instances of cement amounts above 400 Kg/m³, but they do exist.

2. Slag:

The distribution of slag is heavily right-skewed, with a significant number of observations having zero or very low slag content. This suggests that in many mixtures, slag is either not used or used in small amounts.

3. Fly Ash:

Similar to slag, fly ash also has a right-skewed distribution with many data points at or near zero. This indicates that fly ash is either absent or present in small quantities in most mixtures.

4. Water:

The distribution of water is relatively normal, peaking around 180-190 Kg/m³. This suggests that water content in the mixtures is somewhat consistent, with most mixtures having water content within a specific range.

5. Superplasticizer:

The distribution of superplasticizer is heavily right-skewed, with most values near zero. This indicates that superplasticizer is used sparingly in most mixtures, with only a few cases where larger amounts are added.

6. Coarse Aggregate:

The distribution of coarse aggregate appears to be approximately normal, centered around 950-1000 Kg/m³. This indicates consistency in the amount of coarse aggregate used across mixtures.

7. Fine Aggregate:

The distribution of fine aggregate also shows a near-normal distribution, centered around 750-800 Kg/m³. This suggests that the fine aggregate content is relatively consistent across different mixtures.

8. Age:

The age of concrete at the time of testing shows a bimodal distribution, with peaks around 28 days and 365 days. This reflects common testing practices, where concrete is often tested at these specific ages.

9. Strength:

The strength distribution is approximately normal, with a peak around 30-40 MPa. This suggests that the majority of concrete mixtures achieve this level of compressive strength.

#### Implications for Data Preprocessing and Modeling

1. Skewness:

Variables such as slag, fly ash, and superplasticizer show significant skewness. This could pose issues for linear regression models, which assume normally distributed variables. To address this, we might consider applying transformations such as the logarithm or square root to these variables to reduce skewness and stabilize variance.

2. Normal Distribution:

Cement, water, coarse aggregate, fine aggregate, and strength show distributions that are closer to normal. These variables are less likely to require transformation and can be used directly in modeling.

3. Consistency Across Aggregates:

The near-normal distribution of both coarse and fine aggregates suggests that these variables are stable and consistent, which is beneficial for modeling as they are less likely to introduce noise or variability.

4. Bimodal Distribution of Age:

The bimodal distribution of age suggests that concrete strength is often tested at specific time intervals. This could lead to clustering in the data, which might need to be accounted for in the modeling process, possibly by introducing interaction terms or using age as a categorical variable.

5. Transformation and Scaling:

Given the skewness in some variables, transformations should be considered to improve the model's performance. Additionally, scaling might be necessary to ensure that all variables contribute equally to the model, especially when using algorithms sensitive to variable magnitude, such as linear regression.


# Model Fitting and Evaluation

## Cross-Validation Setup
```{r}
# Splitting the data into features (X) and target (y)
X_train <- train_filtered[, c("cement", "slag", "flyash", "water", "super_plast", "coarse_agg", "fine_agg", "age")]
y_train <- train_filtered$strength

# Splitting the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(X_train), 0.8 * nrow(X_train))
X_train_split <- X_train[train_indices, ]
y_train_split <- y_train[train_indices]
X_val_split <- X_train[-train_indices, ]
y_val_split <- y_train[-train_indices]

# Correctly build the train_fold and val_fold data frames
train_fold <- X_train_split  # Use the pre-split training features
train_fold$strength <- y_train_split  # Add the target variable to the training fold

val_fold <- X_val_split  # Use the pre-split validation features
val_fold$strength <- y_val_split  # Add the target variable to the validation fold

# Ensure that the data frames have the correct structure
str(train_fold)
str(val_fold)

# Ensure the 'id' column is not present (if needed, just in case)
if ("id" %in% colnames(train_fold)) {
  train_fold <- train_fold[, !colnames(train_fold) %in% "id"]
}
if ("id" %in% colnames(val_fold)) {
  val_fold <- val_fold[, !colnames(val_fold) %in% "id"]
}

# Verify the structure again
str(train_fold)
str(val_fold)
```
1. Splitting the Data into Features and Target

- X_train: This line extracts the feature columns (cement, slag, flyash, water, super_plast, coarse_agg, fine_agg, and age) from the train_filtered dataset. These are the predictors that will be used to model the target variable.

- y_train: This line extracts the strength column, which is the target variable we aim to predict.

2. Splitting the Data into Training and Validation Sets

- set.seed(123): This sets the seed for random number generation, ensuring that the data split is reproducible. The same seed will produce the same split each time the code is run.

- train_indices: This line randomly selects 80% of the indices from X_train. These indices correspond to the rows that will be used for training.

- X_train_split and y_train_split: These subsets contain the 80% of data (both features and target) selected for training.

- X_val_split and y_val_split: These subsets contain the remaining 20% of the data for validation.

3. Correctly Building train_fold and val_fold Data Frames

- train_fold: This combines the features and the target variable into a single data frame for the training set.

- X_train_split provides the features.

- y_train_split adds the target variable (strength) to the training data frame.

- val_fold: Similarly, this combines the features and the target variable for the validation set.

4. Ensuring the Correct Structure of train_fold and val_fold

str(): This function displays the structure of the data frames. It’s used here to verify that both train_fold and val_fold have the expected structure (i.e., correct number of rows and columns, and correct data types).

5. Removing the id Column (if Present)

- Check for id: This code checks if an id column is present in train_fold and val_fold. If it is, the column is removed.

- Why remove id: The id column is not a predictive feature and should not be used in modeling. It’s typically a unique identifier for each observation and doesn’t contribute to the prediction of strength.

6. Final Verification of Data Structure

Final Check: The structure of train_fold and val_fold is checked again to ensure that the id column has been removed (if it existed) and that the data frames are properly constructed with the expected features and target variable.

## Linear Regression Model - Initial
```{r}
# Fit a linear regression model
model <- lm(strength ~ ., data = train_fold)

# Ensure val_fold is a data frame
val_fold <- as.data.frame(val_fold)

# Make predictions on the validation fold
predictions <- predict(model, newdata = val_fold)

# Calculate MAE and R2 on validation data
mae <- mean(abs(predictions - val_fold$strength))
r2 <- 1 - sum((val_fold$strength - predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

# Print results
cat("Linear Regression MAE:", mae, "\n")
cat("Linear Regression R-squared:", r2, "\n")
```
### Analysis of Initial Model Performance

1. Interpretation of MAE:

The MAE of 6.895671 is reasonably low, suggesting that the model's predictions are relatively close to the actual values. However, there may still be room for improvement by refining the model or applying transformations to the data.

2. Interpretation of R-squared:

An R² of 0.6512115 is a decent starting point, indicating that the model explains a significant portion of the variance in the target variable. However, the remaining 35% of the variance suggests that there are other factors or non-linear relationships that the current model does not capture.

## Feature Transformation
```{r}
# Apply square root transformation to age, as an example
train_fold$sqrt_age <- sqrt(train_fold$age)
val_fold$sqrt_age <- sqrt(val_fold$age)

# Rebuild the model using the transformed feature
model_transformed <- lm(strength ~ . - age + sqrt_age, data = train_fold)
predictions_transformed <- predict(model_transformed, newdata = val_fold)

# Calculate MAE and R2 on validation data with transformed features
transformed_mae <- mean(abs(predictions_transformed - val_fold$strength))
transformed_r2 <- 1 - sum((val_fold$strength - predictions_transformed)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

# Print results
cat("Transformed Linear Regression MAE:", transformed_mae, "\n")
cat("Transformed Linear Regression R-squared:", transformed_r2, "\n")
```

## Feature Engineering
```{r}
# Create an interaction term between cement and super_plast
train_fold$cement_superplast_interaction <- train_fold$cement * train_fold$super_plast
val_fold$cement_superplast_interaction <- val_fold$cement * val_fold$super_plast

# Rebuild the model with the interaction term
model_interaction <- lm(strength ~ . + cement_superplast_interaction, data = train_fold)
predictions_interaction <- predict(model_interaction, newdata = val_fold)

# Calculate MAE and R2 with the interaction term
interaction_mae <- mean(abs(predictions_interaction - val_fold$strength))
interaction_r2 <- 1 - sum((val_fold$strength - predictions_interaction)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

# Print results
cat("Interaction Term Linear Regression MAE:", interaction_mae, "\n")
cat("Interaction Term Linear Regression R-squared:", interaction_r2, "\n")
```

# Residual Analysis and Assumption Checking
```{r}
# Residuals from the latest model (interaction model)
residuals_interaction <- val_fold$strength - predictions_interaction

# 1. Linearity Check
plot(predictions_interaction, val_fold$strength, 
     xlab = "Predicted Strength", ylab = "Actual Strength", 
     main = "Linearity Check")
abline(0, 1, col = "blue")
```
```{r}
# 2. Residuals Check (for homoscedasticity and pattern)
plot(predictions_interaction, residuals_interaction, 
     xlab = "Predicted Strength", ylab = "Residuals", 
     main = "Homoscedasticity and Residuals Check")
abline(h = 0, col = "red")
```
```{r}
# 3. Normality of Residuals
hist(residuals_interaction, 
     main = "Histogram of Residuals", 
     xlab = "Residuals")
# Q-Q plot for residuals to check normality
qqnorm(residuals_interaction)
qqline(residuals_interaction, col = "red")
```

```{r}
# 4. Independence of Residuals
plot(1:length(residuals_interaction), residuals_interaction, 
     xlab = "Observation Number", ylab = "Residuals", 
     main = "Independence of Residuals Check")
abline(h = 0, col = "red")
```

```{r}
# 5. Multicollinearity Check (Variance Inflation Factor - VIF)
vif_values <- vif(model_interaction)
print(vif_values)
```

### Based on the residual and assumption checks:

1. Linearity Check

- The linearity assumption requires that the relationship between the independent variables and the dependent variable (concrete strength) is linear. The plot shows the actual strength on the y-axis and the predicted strength on the x-axis.

- The blue line represents a perfect prediction (where actual equals predicted). Points closely aligned with this line indicate a good fit.

- The points are relatively well aligned, suggesting that the linearity assumption holds, albeit with some deviations at extreme values.

2. Homoscedasticity and Residuals Check

- Homoscedasticity means that the variance of residuals (errors) should be constant across all levels of the independent variables.

- In the plot, residuals (differences between actual and predicted values) are plotted against predicted values.

- If the residuals are randomly dispersed around the horizontal line (y = 0), homoscedasticity is satisfied.

- In this  plot, the residuals appear randomly scattered without a clear pattern, indicating that homoscedasticity is likely satisfied.

3. Normality of Residuals

- The normality of residuals assumption means that the residuals should be normally distributed.

- The histogram provides a visual representation of the distribution of residuals.

- In this histogram, the residuals seem to be roughly normally distributed, though with some slight skewness or kurtosis, which is not uncommon in real-world data.

The Q-Q plot further tests the normality of residuals by comparing them to a theoretical normal distribution.

- Points that closely follow the red line suggest that the residuals are normally distributed.

- In this Q-Q plot, most points lie along the line, supporting the normality assumption, though some deviations at the tails might suggest mild departures from normality.

4. Independence of Residuals

- Independence of residuals implies that the residuals for one observation should not be correlated with the residuals for another.

- This plot shows the residuals across all observations in the order they appear in the dataset.

- If there is no clear pattern or trend in the residuals, the assumption of independence is met.

- The plot does not show any obvious patterns, indicating that the residuals are likely independent.

5. Multicollinearity Check (Variance Inflation Factor - VIF)

- Multicollinearity occurs when two or more predictor variables are highly correlated, leading to unreliable coefficient estimates.

- VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity.

- VIF values above 10 are generally considered problematic.

- The VIF values for some predictors, especially super_plast and the interaction term, are somewhat high, which might suggest some degree of multicollinearity, particularly due to the interaction term. This can be an area for further exploration or model adjustment.


### Refining the Model by Addressing Multicollinearity and Improving Assumptions

1. Addressing Multicollinearity

1.1. Remove or Combine Highly Correlated Predictors:

If you find two or more predictors that are highly correlated (e.g., VIF > 10), consider removing one of them or combining them into a single predictor.

```{r}
# Check correlation between cement and super_plast
cor(train_fold$cement, train_fold$super_plast)

# If highly correlated, consider removing one or creating a composite variable
train_fold$cement_superplast_composite <- train_fold$cement + train_fold$super_plast
val_fold$cement_superplast_composite <- val_fold$cement + val_fold$super_plast

# Update model to use composite instead of individual terms
model_refined <- lm(strength ~ . + cement_superplast_composite, data = train_fold)

# Evaluate the new model
predictions_refined <- predict(model_refined, newdata = val_fold)
refined_mae <- mean(abs(predictions_refined - val_fold$strength))
refined_r2 <- 1 - sum((val_fold$strength - predictions_refined)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Refined Model MAE:", refined_mae, "\n")
cat("Refined Model R-squared:", refined_r2, "\n")
```

1.2. Regularization Techniques:

- Ridge Regression or Lasso Regression can help handle multicollinearity by adding a penalty to the coefficients.

- Ridge regression shrinks coefficients, while Lasso can set some coefficients to zero, effectively performing variable selection.

```{r}
library(glmnet)

# Prepare data matrix for glmnet (Ridge requires matrix input)
X <- model.matrix(strength ~ ., train_fold)[, -1]  # Remove intercept term
y <- train_fold$strength

# Fit Ridge Regression model
ridge_model <- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge
ridge_cv <- cv.glmnet(X, y, alpha = 0)
best_lambda <- ridge_cv$lambda.min  # Optimal lambda

# Predict on validation data
X_val <- model.matrix(strength ~ ., val_fold)[, -1]
ridge_predictions <- predict(ridge_model, s = best_lambda, newx = X_val)
ridge_mae <- mean(abs(ridge_predictions - val_fold$strength))
ridge_r2 <- 1 - sum((val_fold$strength - ridge_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Ridge Regression MAE:", ridge_mae, "\n")
cat("Ridge Regression R-squared:", ridge_r2, "\n")
```
2. Transforming the Target Variable or Features

2.1. Transform the Target Variable (Strength):

- If the residuals are not normally distributed, you might consider transforming the target variable.

- Log Transformation: Helps stabilize variance and make the data more normal.

- Square Root Transformation: Useful for reducing skewness.

```{r}
# Log transformation of the target variable
train_fold$log_strength <- log(train_fold$strength)
val_fold$log_strength <- log(val_fold$strength)

# Rebuild the model
model_log_transformed <- lm(log_strength ~ . + cement_superplast_interaction, data = train_fold)

# Predict and evaluate on log scale
predictions_log <- predict(model_log_transformed, newdata = val_fold)
log_mae <- mean(abs(exp(predictions_log) - val_fold$strength))
log_r2 <- 1 - sum((val_fold$strength - exp(predictions_log))^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Log-Transformed Model MAE:", log_mae, "\n")
cat("Log-Transformed Model R-squared:", log_r2, "\n")
```

2.2. Transform Predictors:

- Similar transformations can be applied to the predictors to improve linearity and homoscedasticity.

- Box-Cox Transformation: Automatically finds the best transformation for your data.

```{r}
library(MASS)

# Apply Box-Cox transformation to a predictor
box_cox_transform <- boxcox(lm(strength ~ ., data = train_fold), lambda = seq(-2, 2, 0.1))

# Choose the best lambda
best_lambda <- box_cox_transform$x[which.max(box_cox_transform$y)]
train_fold$transformed_cement <- (train_fold$cement^best_lambda - 1) / best_lambda
val_fold$transformed_cement <- (val_fold$cement^best_lambda - 1) / best_lambda

# Rebuild the model
model_boxcox <- lm(strength ~ . + transformed_cement, data = train_fold)

# Evaluate the model
predictions_boxcox <- predict(model_boxcox, newdata = val_fold)
boxcox_mae <- mean(abs(predictions_boxcox - val_fold$strength))
boxcox_r2 <- 1 - sum((val_fold$strength - predictions_boxcox)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Box-Cox Transformed Model MAE:", boxcox_mae, "\n")
cat("Box-Cox Transformed Model R-squared:", boxcox_r2, "\n")
```

### Result Interpretation
1. Refined Model:

- MAE: 5.428254

- R-squared: 0.8299967

The refined model, likely after addressing some multicollinearity issues or introducing interaction terms, shows significant improvement compared to the initial models. An R-squared of ~0.83 suggests that around 83% of the variance in the concrete's compressive strength can be explained by the model's predictors.

2. Ridge Regression:

- MAE: 5.976078

- R-squared: 0.7810819

Ridge regression, which is designed to handle multicollinearity by shrinking coefficients, resulted in a slightly higher MAE and a lower R-squared compared to the refined model. This indicates that while Ridge Regression addresses multicollinearity, it may not be as effective in this context as other methods for this specific dataset. Ridge regression tends to shrink coefficients toward zero, which may lead to a loss in model flexibility, thereby increasing error.

3. Log-Transformed Model:

- MAE: 4.283084

- R-squared: 0.8303681

Applying a log transformation to the target variable further reduced the MAE and slightly improved the R-squared. This suggests that the log transformation helped stabilize variance and reduce the impact of outliers, improving the model's predictive accuracy. The improvement in R-squared, though modest, indicates a more accurate model fit.

4. Box-Cox Transformed Model:

- MAE: 3.489857

- R-squared: 0.9251018

The Box-Cox transformation provided the most significant improvement, drastically reducing the MAE and increasing the R-squared to 0.93. This transformation is particularly powerful as it automatically finds the best transformation to normalize the data and stabilize variance. The substantial improvement suggests that the Box-Cox transformation effectively addressed non-linearity and heteroscedasticity, leading to a highly predictive and well-fitting model.

## Second Residual Analysis and Assumption Checking
```{r}
residuals_boxcox <- val_fold$strength - predictions_boxcox

# 1. Linearity Check
plot(predictions_boxcox, val_fold$strength, 
     xlab = "Predicted Strength", 
     ylab = "Actual Strength", 
     main = "Linearity Check")
abline(0, 1, col = "blue")  # Adding a reference line for better visualization

# 2. Homoscedasticity Check
plot(predictions_boxcox, residuals_boxcox, 
     xlab = "Predicted Strength", 
     ylab = "Residuals", 
     main = "Homoscedasticity and Residuals Check")
abline(h = 0, col = "red")  # Adding a horizontal line at zero for reference

# 3. Normality of Residuals
hist(residuals_boxcox, 
     main = "Histogram of Residuals", 
     xlab = "Residuals", 
     breaks = 20)  # Histogram to check the normality of residuals

# Q-Q Plot for Residuals
qqnorm(residuals_boxcox, 
       main = "Normal Q-Q Plot")
qqline(residuals_boxcox, col = "red")  # Adding a reference line for normality check

# 4. Independence of Residuals
plot(1:length(residuals_boxcox), residuals_boxcox, 
     xlab = "Observation Number", 
     ylab = "Residuals", 
     main = "Independence of Residuals Check")
abline(h = 0, col = "red")  # Horizontal line at zero
```

```{r}
alias(model_boxcox)
```

```{r}
# Remove the cement_superplast_composite variable
model_boxcox_refined <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + sqrt_age + cement_superplast_interaction + log_strength + transformed_cement, data = train_fold)
```

```{r}
# 5. Multicollinearity Check (Variance Inflation Factor)
vif_values <- vif(model_boxcox_refined)
print(vif_values)
```

```{r}
# Refine the model by removing transformed_cement
model_boxcox_refined_v2 <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + sqrt_age + cement_superplast_interaction + log_strength, data = train_fold)

# Check VIF again
vif_values_v2 <- vif(model_boxcox_refined_v2)
print(vif_values_v2)
```
```{r}
# Refine the model further by removing sqrt_age
model_boxcox_refined_v3_no_log <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, data = train_fold)


# Check VIF again
vif_values_v3 <- vif(model_boxcox_refined_v3_no_log)
print(vif_values_v3)
```
```{r}
# Rebuild the model excluding log_strength for predictions
model_test_refined <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, data = train_fold)
```

## Refine the model

```{r}
# Prepare data matrix for glmnet (Ridge requires matrix input)
X <- model.matrix(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, train_fold)[, -1]
y <- train_fold$strength

# Ridge Regression with Cross-Validation
set.seed(123)
ridge_cv <- cv.glmnet(X, y, alpha = 0)
ridge_best_lambda <- ridge_cv$lambda.min
ridge_model <- glmnet(X, y, alpha = 0, lambda = ridge_best_lambda)

# Prepare validation set matrix (ensure the same transformations are applied)
X_val <- model.matrix(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, val_fold)[, -1]

# Make predictions using Ridge model
ridge_predictions <- predict(ridge_model, newx = X_val, s = ridge_best_lambda)

# Evaluate predictions
ridge_mae <- mean(abs(ridge_predictions - val_fold$strength))
ridge_r2 <- 1 - sum((val_fold$strength - ridge_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

# Print results
cat("Ridge Regression MAE:", ridge_mae, "\n")
cat("Ridge Regression R-squared:", ridge_r2, "\n")
```


```{r}
# Lasso Regression with Cross-Validation
set.seed(123)
lasso_cv <- cv.glmnet(X, y, alpha = 1)
lasso_best_lambda <- lasso_cv$lambda.min
lasso_model <- glmnet(X, y, alpha = 1, lambda = lasso_best_lambda)

# Make predictions using Lasso model
lasso_predictions <- predict(lasso_model, newx = X_val, s = lasso_best_lambda)

# Evaluate predictions
lasso_mae <- mean(abs(lasso_predictions - val_fold$strength))
lasso_r2 <- 1 - sum((val_fold$strength - lasso_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

# Print results
cat("Lasso Regression MAE:", lasso_mae, "\n")
cat("Lasso Regression R-squared:", lasso_r2, "\n")
```

```{r}
# Include polynomial features for selected variables
poly_features <- poly(train_fold$cement, degree = 2, raw = TRUE)
train_fold$cement_poly1 <- poly_features[, 1]
train_fold$cement_poly2 <- poly_features[, 2]

# Apply the same transformations to validation set
val_poly_features <- poly(val_fold$cement, degree = 2, raw = TRUE)
val_fold$cement_poly1 <- val_poly_features[, 1]
val_fold$cement_poly2 <- val_poly_features[, 2]

# Build a model with polynomial features
model_poly <- lm(strength ~ cement_poly1 + cement_poly2 + slag + flyash + water + super_plast + coarse_agg + fine_agg + age, data = train_fold)
poly_predictions <- predict(model_poly, newdata = val_fold)

# Evaluate the polynomial model
poly_mae <- mean(abs(poly_predictions - val_fold$strength))
poly_r2 <- 1 - sum((val_fold$strength - poly_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Polynomial Regression MAE:", poly_mae, "\n")
cat("Polynomial Regression R-squared:", poly_r2, "\n")
```


```{r}
library(randomForest)

# Remove 'log_strength', 'cement_superplast_composite', and any polynomial features from the training data
train_fold <- train_fold[, !grepl("log_strength|cement_superplast_composite|poly", names(train_fold))]

# Retrain the Random Forest model without unnecessary features
rf_model <- randomForest(strength ~ ., data = train_fold, ntree = 500)

# Evaluate the Random Forest model on the validation data
rf_predictions <- predict(rf_model, newdata = val_fold)

# Calculate MAE and R2 for validation data
rf_mae <- mean(abs(rf_predictions - val_fold$strength))
rf_r2 <- 1 - sum((val_fold$strength - rf_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Random Forest MAE:", rf_mae, "\n")
cat("Random Forest R-squared:", rf_r2, "\n")
```

```{r}
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Define the grid of hyperparameters to tune
# Define the hyperparameter grid
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5, 6, 7, 8))

# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
  strength ~ ., 
  data = train_fold, 
  method = "rf", 
  trControl = train_control, 
  tuneGrid = tune_grid
)

# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)

# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
```

```{r}
# Calculate the residuals for the validation set using the tuned Random Forest model
residuals_rf_tuned <- val_fold$strength - rf_tuned_predictions

# 1. Linearity Check
plot(rf_tuned_predictions, val_fold$strength, 
     xlab = "Predicted Strength", 
     ylab = "Actual Strength", 
     main = "Linearity Check - Tuned Random Forest")
abline(0, 1, col = "blue")  # Line of perfect prediction

# 2. Homoscedasticity Check
plot(rf_tuned_predictions, residuals_rf_tuned, 
     xlab = "Predicted Strength", 
     ylab = "Residuals", 
     main = "Homoscedasticity and Residuals Check - Tuned Random Forest")
abline(h = 0, col = "red")  # Reference line for residuals at zero

# 3. Normality of Residuals
hist(residuals_rf_tuned, 
     main = "Histogram of Residuals - Tuned Random Forest", 
     xlab = "Residuals")
qqnorm(residuals_rf_tuned)
qqline(residuals_rf_tuned, col = "red")  # Reference line for normality

# 4. Independence of Residuals
plot(1:length(residuals_rf_tuned), residuals_rf_tuned, 
     xlab = "Observation Number", 
     ylab = "Residuals", 
     main = "Independence of Residuals Check - Tuned Random Forest")
abline(h = 0, col = "red")  # Reference line at zero
```

```{r}
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Define the grid of hyperparameters to tune (only mtry for Random Forest)
tune_grid <- expand.grid(mtry = c(2, 4, 6, 8))

# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
  strength ~ ., 
  data = train_fold, 
  method = "rf", 
  trControl = train_control, 
  tuneGrid = tune_grid,
  ntree = 500,  # Set fixed ntree value here
  importance = TRUE
)

# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)

# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)

cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
```

# Test Prediction

```{r}
test <- read.csv("data/data-test.csv")
str(test)
```


```{r}
# Apply necessary transformations to the test data, ensuring no 'log_strength', 'cement_superplast_composite', or polynomial features
test$cement_superplast_interaction <- test$cement * test$super_plast
test$sqrt_age <- sqrt(test$age)
test$transformed_cement <- sqrt(test$cement)

# Ensure the log_strength and cement_superplast_composite columns are not present
test <- test[, !names(test) %in% c("log_strength", "cement_superplast_composite")]

# Confirm that the structure of the test data is correct
str(test)
```

```{r}
# Generate predictions from the Box-Cox transformed linear regression model
test_predictions_boxcox <- predict(model_boxcox_refined_v3_no_log, newdata = test)

# Generate predictions from the tuned Random Forest model
test_predictions_rf <- predict(rf_tuned_model, newdata = test)

# Ensemble the predictions using a weighted average
# You can adjust the weights as needed (e.g., 0.5 for each model)
ensemble_weights <- c(0.5, 0.5)  # Equal weighting for both models
test_predictions_ensemble <- ensemble_weights[1] * test_predictions_boxcox + 
                             ensemble_weights[2] * test_predictions_rf

# Ensure the length of predictions matches the number of rows in the test dataset
print(length(test_predictions_ensemble))  # Should be 205
print(nrow(test))  # Should be 205
```


```{r}
# Check if the id column exists
if ("id" %in% colnames(test)) {
    print("id column is present")
} else {
    print("id column is missing")
}
```

```{r}
# Store the ensemble predictions in a data frame
submission_df <- data.frame(id = test$id, strength = test_predictions_ensemble)

# Print the ensemble predictions to review them
print(submission_df)
```

```{r}
# Save the ensemble predictions to a CSV file for submission
write.csv(submission_df, file = "test_predictions_ensemble_submission.csv", row.names = FALSE)
```

```{r}
# Interpret the Box-Cox Transformed Linear Regression Model
summary(model_boxcox_refined_v3_no_log)

# Interpret the Variable Importance from Random Forest
importance(rf_tuned_model$finalModel)
```

**1. Interpreting the Predictors from the Box-Cox Transformed Linear Regression Model**

Effect of Each Predictor: 

The magnitude and sign of the estimated coefficients provide insights into the direction and extent of the effect of each predictor on concrete compression strength. For instance, positive coefficients indicate a positive relationship with strength, while negative coefficients suggest a negative impact.

Significance of Predictors: 

The significance of each predictor is assessed by its p-value, which tests the null hypothesis that the coefficient is zero (no effect). A p-value below a certain threshold (commonly 0.05) indicates that the predictor has a statistically significant effect on concrete strength.

Standard Error Interpretation: 

The standard error measures the variability or uncertainty in the estimated coefficients. Smaller standard errors indicate more precise estimates, while larger standard errors suggest more uncertainty in the coefficient.

**Linear Regression Model (Box-Cox Transformed) – Predictor Breakdown**

- Cement: 

For each additional unit of cement, the concrete compressive strength increases by approximately 0.11 MPa. Cement is a highly significant predictor of strength, as indicated by a p-value of less than 2e-16. This result aligns with the role of cement as a primary binding material in concrete.

- Slag: 

Every additional unit of slag increases concrete strength by 0.079 MPa. Slag is also a highly significant predictor (p-value < 7.73e-10), contributing to the overall strength of the concrete mix.

- Flyash: 

Flyash has a significant positive impact on concrete strength, with an estimated increase of 0.059 MPa per unit. This predictor is significant (p-value < 0.000143), suggesting that it plays a crucial role in enhancing concrete strength when used appropriately.

- Water: 

Water has a negative effect on concrete strength. Each additional unit of water decreases strength by 0.235 MPa. This result is statistically significant (p-value < 1.97e-05) and reflects the known relationship that excessive water content in the mix weakens concrete.

- Superplasticizer: 

Superplasticizer increases concrete strength by approximately 0.926 MPa per unit, making it a significant contributor to strength (p-value < 0.000617). Superplasticizers improve workability without adding excess water, which helps in maintaining and enhancing compressive strength.

- Coarse Aggregate: 

Coarse aggregate has a very minimal and statistically insignificant effect on strength (p-value = 0.937). Its impact is negligible in this model.

- Fine Aggregate: 

Similarly, fine aggregate does not significantly impact concrete strength, with a p-value of 0.942. Its effect is statistically insignificant in this linear regression model.

- Age: 

Age has a substantial and highly significant effect on strength, with each additional day contributing 0.133 MPa to compressive strength (p-value < 2e-16). Concrete gains strength as it cures over time, making age one of the most critical factors.

- Cement-Superplasticizer Interaction: 

The interaction between cement and superplasticizer has a slight negative effect on strength, decreasing it by 0.00188 MPa for each unit of the interaction term. This predictor is significant (p-value = 0.006), indicating that the combined effect of these two components might have diminishing returns beyond certain levels.


**Measuring the Effect of Each Predictor:**

The coefficient value represents the estimated change in concrete strength (in MPa) for each one-unit change in the predictor, holding other variables constant. For example:

- Cement: 

A 1-unit increase in cement content corresponds to an increase in concrete strength by 0.11 MPa.

- Water: 

A 1-unit increase in water reduces the strength by 0.235 MPa.

**Interpreting Standard Error:**

The standard error reflects the variability in the estimate of each coefficient. A smaller standard error means the estimate is more precise. For instance, cement has a standard error of 0.012, indicating that its effect is estimated with high precision. On the other hand, coarse aggregate has a relatively larger standard error (0.0119), but its small coefficient makes it statistically insignificant.

**Significance of Predictors:**

- Predictors with p-values less than 0.05 are considered statistically significant. For example:

- Cement, slag, flyash, water, superplasticizer, and age are significant predictors of strength (all with p-values below 0.05).
Coarse aggregate and fine aggregate are not significant, as their p-values are greater than 0.05, meaning their contributions to predicting strength are negligible in this model.

**2. Interpreting the Variable Importance from Random Forest**

In Random Forest, variable importance is typically measured by two metrics: %IncMSE (percentage increase in Mean Squared Error) and IncNodePurity (Increase in Node Purity).

- %IncMSE: 

This measures how much the Mean Squared Error increases when a particular predictor is excluded from the model. A higher value indicates that the predictor is more important for making accurate predictions.

- IncNodePurity: 

This measures the total increase in node purity (reduction in variance) brought about by splits on the variable. Higher values indicate greater importance in splitting the data.


**Random Forest Model – Important Predictors Breakdown**

- Cement: 

Cement is one of the most important predictors in the Random Forest model, contributing significantly to reducing the model's error. It plays a crucial role in predicting concrete compressive strength, confirming its fundamental impact on the final mix.

- Slag: 

Slag ranks highly in importance, contributing substantially to the accuracy of the model. Its ability to enhance concrete strength is well captured by the Random Forest model, aligning with its positive influence observed in the linear regression.

- Flyash: 

Flyash, though less impactful than cement or slag, still plays an important role in the model's predictions. It moderately affects the strength of concrete, particularly in mixes where it's used as a supplementary material.

- Water: 

Water shows strong importance in the Random Forest model, highlighting its role in determining strength. Too much water weakens concrete, while the right amount ensures proper hydration, making water a critical factor.

- Superplasticizer: 

The contribution of superplasticizer is significant, though not as strong as cement or slag. It helps improve workability and indirectly contributes to strength by reducing the need for excess water.

- Coarse Aggregate: 

Coarse aggregate holds moderate importance in the model, playing a supporting role in the structure of the concrete. However, its contribution to strength is less prominent compared to other ingredients.

- Fine Aggregate: 

Fine aggregate has moderate importance in the Random Forest model, contributing to the balance and consistency of the mix, though it does not have a strong direct effect on compressive strength.

- Age: 

Age is a critical predictor, contributing significantly to the strength of concrete. The longer the concrete cures, the stronger it becomes, and this relationship is captured well by the Random Forest model.

- Cement-Superplasticizer Interaction: 

This interaction term is highly important, suggesting that there are complex relationships between cement and superplasticizer that influence the final strength of the concrete. The model captures these intricate effects effectively.

**Summary of Interpretation:**

- Key Significant Predictors: 

Cement, slag, flyash, water, superplasticizer, and age are highly significant in predicting concrete compression strength in both models.

- Interaction Effect: 

The interaction between cement and superplasticizer has a slight negative impact but remains significant.

- Random Forest Importance: 

Random Forest confirms the importance of cement, slag, age, and the interaction term in predicting strength.


# Finding the Right Material Composition

```{r}
# Perform ANOVA for key ingredients (cement, slag, flyash, water, super_plast)
anova_result <- aov(strength ~ cement + slag + flyash + water + super_plast, data = train_filtered)
summary(anova_result)

# Perform T-test for age groups (<=28 days and >28 days)
young <- subset(train_filtered, age <= 28)
old <- subset(train_filtered, age > 28)
ttest_result <- t.test(young$strength, old$strength)
print(ttest_result)

# Find the combination with the maximum mean strength across key ingredients and age
mean_strength <- aggregate(strength ~ cement + slag + flyash + water + super_plast + age, 
                           data = train_filtered, FUN = mean)
max_mean_strength <- mean_strength[which.max(mean_strength$strength), ]
print(max_mean_strength)
```


#### Results

**1. T-test for Age:**

- The p-value remains extremely small (< 2.2e-16), confirming a statistically significant difference in mean strength between "young" and "old" concrete.

- The confidence interval (-19.59 to -15.19) suggests that older concrete (mean strength = 48.31 MPa) is significantly stronger than younger concrete (mean strength = 30.92 MPa).

- Interpretation: Concrete strength increases significantly with age, underscoring the importance of curing time in achieving higher compressive strength. Concrete aged beyond 28 days is notably stronger, which aligns with industry expectations.

**2. Optimal Composition from Mean Strength Calculation:**

- The highest mean concrete strength of 79.3 MPa is achieved with the following composition:

- Cement: 362.6 kg/m³
- Fly Ash: 0 kg/m³
- Superplasticizer: 11.6 kg/m³
- Water: 164.9 kg/m³
- Age: 91 days

- Interpretation: This optimal composition maximizes compressive strength. The absence of fly ash suggests that, in this dataset, fly ash may not contribute positively to strength. Superplasticizer and water ratios are balanced to improve workability while maintaining strength. The extended curing time of 91 days further enhances the strength to its maximum potential.

**3. Overall Insight**

- Impact of Age: 

The strong significance of age on strength, as revealed by the T-test, confirms that extended curing times (beyond 28 days) are critical for developing maximum concrete strength. This emphasizes the need for adequate curing in construction practices to ensure durability and performance.

- Optimal Material Composition: 

The identified mix (362.6 kg of cement, 11.6 kg of superplasticizer, 0 kg of fly ash, 164.9 kg of water, cured for 91 days) is optimal for maximizing strength in this specific dataset. This composition is ideal for high-performance concrete applications, where strength is the primary objective.


# Conclusion

The goal of this capstone project was to develop a model that accurately predicts the compressive strength of concrete based on its ingredients and age, and to identify the optimal composition for maximizing strength. Through a systematic approach involving data preprocessing, exploratory data analysis, model building, and statistical testing, we successfully achieved this goal.

1. Goal Achievement:

- The primary goal of predicting concrete compressive strength was achieved with a high degree of accuracy. The ensemble model, which combined predictions from the Box-Cox transformed linear regression model and a Random Forest model, demonstrated strong predictive performance with a Mean Absolute Error (MAE) of 3.48 and an R-squared value of 0.93 on the validation dataset. These metrics indicate that the model is both precise and reliable in predicting concrete strength.

2. Machine Learning for Problem-Solving:

- This project demonstrates that machine learning can effectively solve the problem of predicting concrete compressive strength. By leveraging regression techniques and advanced machine learning methods such as Random Forests, we were able to model the complex relationships between ingredients, age, and the resulting strength. The use of transformations, interaction terms, and ensemble methods further enhanced the model’s ability to capture non-linear relationships and improve accuracy.

3. Model Used and Performance:

- We employed an ensemble approach combining the strengths of a Box-Cox transformed linear regression model and a Random Forest model to address non-linearity, multicollinearity, and other model assumptions. The final ensemble model exhibited excellent performance metrics:

- MAE: 3.49

- R-squared: 0.93

with result that implement to test dataset:

- MAE: 7.36

- R-squared: 0.79

- These results demonstrate that the ensemble model is capable of making highly accurate predictions, which is crucial for optimizing concrete mix designs.

4. Potential Business Implementation:

- The insights gained from this project have significant implications for the construction industry. By identifying the optimal mix of concrete ingredients, companies can produce stronger and more durable concrete, leading to improved structural integrity and longer lifespans for buildings and infrastructure. This optimization enhances safety and offers cost savings by reducing the need for excessive materials.

- The model developed in this project can be integrated into concrete mix design software, enabling engineers to input ingredient quantities and receive real-time predictions of compressive strength. This tool can help streamline the mix design process, ensuring that the most effective combinations are used for specific construction needs, thereby improving efficiency and reducing material waste.

In conclusion, this capstone project successfully addressed the challenge of predicting and optimizing concrete compressive strength using an ensemble of machine learning models. The final model's strong performance and the identification of an optimal mix composition provide valuable insights that can be directly applied in real-world construction scenarios, offering both technical and economic benefits.
