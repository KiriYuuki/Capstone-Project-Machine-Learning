train_fold$log_strength <- log(train_fold$strength)
val_fold$log_strength <- log(val_fold$strength)
# Rebuild the model
model_log_transformed <- lm(log_strength ~ . + cement_superplast_interaction, data = train_fold)
# Predict and evaluate on log scale
predictions_log <- predict(model_log_transformed, newdata = val_fold)
log_mae <- mean(abs(exp(predictions_log) - val_fold$strength))
log_r2 <- 1 - sum((val_fold$strength - exp(predictions_log))^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Log-Transformed Model MAE:", log_mae, "\n")
cat("Log-Transformed Model R-squared:", log_r2, "\n")
library(MASS)
# Apply Box-Cox transformation to a predictor
box_cox_transform <- boxcox(lm(strength ~ ., data = train_fold), lambda = seq(-2, 2, 0.1))
# Choose the best lambda
best_lambda <- box_cox_transform$x[which.max(box_cox_transform$y)]
train_fold$transformed_cement <- (train_fold$cement^best_lambda - 1) / best_lambda
val_fold$transformed_cement <- (val_fold$cement^best_lambda - 1) / best_lambda
# Rebuild the model
model_boxcox <- lm(strength ~ . + transformed_cement, data = train_fold)
# Evaluate the model
predictions_boxcox <- predict(model_boxcox, newdata = val_fold)
boxcox_mae <- mean(abs(predictions_boxcox - val_fold$strength))
boxcox_r2 <- 1 - sum((val_fold$strength - predictions_boxcox)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Box-Cox Transformed Model MAE:", boxcox_mae, "\n")
cat("Box-Cox Transformed Model R-squared:", boxcox_r2, "\n")
residuals_boxcox <- val_fold$strength - predictions_boxcox
# 1. Linearity Check
plot(predictions_boxcox, val_fold$strength,
xlab = "Predicted Strength",
ylab = "Actual Strength",
main = "Linearity Check")
abline(0, 1, col = "blue")  # Adding a reference line for better visualization
# 2. Homoscedasticity Check
plot(predictions_boxcox, residuals_boxcox,
xlab = "Predicted Strength",
ylab = "Residuals",
main = "Homoscedasticity and Residuals Check")
abline(h = 0, col = "red")  # Adding a horizontal line at zero for reference
# 3. Normality of Residuals
hist(residuals_boxcox,
main = "Histogram of Residuals",
xlab = "Residuals",
breaks = 20)  # Histogram to check the normality of residuals
# Q-Q Plot for Residuals
qqnorm(residuals_boxcox,
main = "Normal Q-Q Plot")
qqline(residuals_boxcox, col = "red")  # Adding a reference line for normality check
# 4. Independence of Residuals
plot(1:length(residuals_boxcox), residuals_boxcox,
xlab = "Observation Number",
ylab = "Residuals",
main = "Independence of Residuals Check")
abline(h = 0, col = "red")  # Horizontal line at zero
alias(model_boxcox)
# Remove the cement_superplast_composite variable
model_boxcox_refined <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + sqrt_age + cement_superplast_interaction + log_strength + transformed_cement, data = train_fold)
# 5. Multicollinearity Check (Variance Inflation Factor)
vif_values <- vif(model_boxcox_refined)
print(vif_values)
# Refine the model by removing transformed_cement
model_boxcox_refined_v2 <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + sqrt_age + cement_superplast_interaction + log_strength, data = train_fold)
# Check VIF again
vif_values_v2 <- vif(model_boxcox_refined_v2)
print(vif_values_v2)
# Refine the model further by removing sqrt_age
model_boxcox_refined_v3 <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction + log_strength, data = train_fold)
# Check VIF again
vif_values_v3 <- vif(model_boxcox_refined_v3)
print(vif_values_v3)
# Rebuild the model excluding log_strength for predictions
model_test_refined <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, data = train_fold)
# Prepare data matrix for glmnet (Ridge requires matrix input)
X <- model.matrix(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, train_fold)[, -1]
y <- train_fold$strength
# Ridge Regression with Cross-Validation
set.seed(123)
ridge_cv <- cv.glmnet(X, y, alpha = 0)
ridge_best_lambda <- ridge_cv$lambda.min
ridge_model <- glmnet(X, y, alpha = 0, lambda = ridge_best_lambda)
# Prepare validation set matrix (ensure the same transformations are applied)
X_val <- model.matrix(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, val_fold)[, -1]
# Make predictions using Ridge model
ridge_predictions <- predict(ridge_model, newx = X_val, s = ridge_best_lambda)
# Evaluate predictions
ridge_mae <- mean(abs(ridge_predictions - val_fold$strength))
ridge_r2 <- 1 - sum((val_fold$strength - ridge_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
# Print results
cat("Ridge Regression MAE:", ridge_mae, "\n")
cat("Ridge Regression R-squared:", ridge_r2, "\n")
# Lasso Regression with Cross-Validation
set.seed(123)
lasso_cv <- cv.glmnet(X, y, alpha = 1)
lasso_best_lambda <- lasso_cv$lambda.min
lasso_model <- glmnet(X, y, alpha = 1, lambda = lasso_best_lambda)
# Make predictions using Lasso model
lasso_predictions <- predict(lasso_model, newx = X_val, s = lasso_best_lambda)
# Evaluate predictions
lasso_mae <- mean(abs(lasso_predictions - val_fold$strength))
lasso_r2 <- 1 - sum((val_fold$strength - lasso_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
# Print results
cat("Lasso Regression MAE:", lasso_mae, "\n")
cat("Lasso Regression R-squared:", lasso_r2, "\n")
# Include polynomial features for selected variables
poly_features <- poly(train_fold$cement, degree = 2, raw = TRUE)
train_fold$cement_poly1 <- poly_features[, 1]
train_fold$cement_poly2 <- poly_features[, 2]
# Apply the same transformations to validation set
val_poly_features <- poly(val_fold$cement, degree = 2, raw = TRUE)
val_fold$cement_poly1 <- val_poly_features[, 1]
val_fold$cement_poly2 <- val_poly_features[, 2]
# Build a model with polynomial features
model_poly <- lm(strength ~ cement_poly1 + cement_poly2 + slag + flyash + water + super_plast + coarse_agg + fine_agg + age, data = train_fold)
poly_predictions <- predict(model_poly, newdata = val_fold)
# Evaluate the polynomial model
poly_mae <- mean(abs(poly_predictions - val_fold$strength))
poly_r2 <- 1 - sum((val_fold$strength - poly_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Polynomial Regression MAE:", poly_mae, "\n")
cat("Polynomial Regression R-squared:", poly_r2, "\n")
library(randomForest)
# Remove 'log_strength', 'cement_superplast_composite', and any polynomial features from the training data
train_fold <- train_fold[, !grepl("log_strength|cement_superplast_composite|poly", names(train_fold))]
# Retrain the Random Forest model without unnecessary features
rf_model <- randomForest(strength ~ ., data = train_fold, ntree = 500)
# Evaluate the Random Forest model on the validation data
rf_predictions <- predict(rf_model, newdata = val_fold)
# Calculate MAE and R2 for validation data
rf_mae <- mean(abs(rf_predictions - val_fold$strength))
rf_r2 <- 1 - sum((val_fold$strength - rf_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Random Forest MAE:", rf_mae, "\n")
cat("Random Forest R-squared:", rf_r2, "\n")
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
# Define the grid of hyperparameters to tune
# Define the hyperparameter grid
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5, 6, 7, 8))
# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
strength ~ .,
data = train_fold,
method = "rf",
trControl = train_control,
tuneGrid = tune_grid
)
# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)
# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
# Calculate the residuals for the validation set using the tuned Random Forest model
residuals_rf_tuned <- val_fold$strength - rf_tuned_predictions
# 1. Linearity Check
plot(rf_tuned_predictions, val_fold$strength,
xlab = "Predicted Strength",
ylab = "Actual Strength",
main = "Linearity Check - Tuned Random Forest")
abline(0, 1, col = "blue")  # Line of perfect prediction
# 2. Homoscedasticity Check
plot(rf_tuned_predictions, residuals_rf_tuned,
xlab = "Predicted Strength",
ylab = "Residuals",
main = "Homoscedasticity and Residuals Check - Tuned Random Forest")
abline(h = 0, col = "red")  # Reference line for residuals at zero
# 3. Normality of Residuals
hist(residuals_rf_tuned,
main = "Histogram of Residuals - Tuned Random Forest",
xlab = "Residuals")
qqnorm(residuals_rf_tuned)
qqline(residuals_rf_tuned, col = "red")  # Reference line for normality
# 4. Independence of Residuals
plot(1:length(residuals_rf_tuned), residuals_rf_tuned,
xlab = "Observation Number",
ylab = "Residuals",
main = "Independence of Residuals Check - Tuned Random Forest")
abline(h = 0, col = "red")  # Reference line at zero
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
# Define the grid of hyperparameters to tune (only mtry for Random Forest)
tune_grid <- expand.grid(mtry = c(2, 4, 6, 8))
# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
strength ~ .,
data = train_fold,
method = "rf",
trControl = train_control,
tuneGrid = tune_grid,
ntree = 500,  # Set fixed ntree value here
importance = TRUE
)
# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)
# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
test <- read.csv("data/data-test.csv")
str(test)
# Apply necessary transformations to the test data, ensuring no 'log_strength', 'cement_superplast_composite', or polynomial features
test$cement_superplast_interaction <- test$cement * test$super_plast
test$sqrt_age <- sqrt(test$age)
test$transformed_cement <- sqrt(test$cement)
# Ensure the log_strength and cement_superplast_composite columns are not present
test <- test[, !names(test) %in% c("log_strength", "cement_superplast_composite")]
# Confirm that the structure of the test data is correct
str(test)
# Apply necessary transformations to the test data, ensuring no 'log_strength', 'cement_superplast_composite', or polynomial features
test$cement_superplast_interaction <- test$cement * test$super_plast
test$sqrt_age <- sqrt(test$age)
test$transformed_cement <- sqrt(test$cement)
# Ensure the log_strength and cement_superplast_composite columns are not present
test <- test[, !names(test) %in% c("log_strength", "cement_superplast_composite")]
# Confirm that the structure of the test data is correct
str(test)
# Generate predictions from the Box-Cox transformed linear regression model
test_predictions_boxcox <- predict(model_boxcox_refined_v3, newdata = test)
test <- read.csv("data/data-test.csv")
str(test)
# Apply necessary transformations to the test data, ensuring no 'log_strength', 'cement_superplast_composite', or polynomial features
test$cement_superplast_interaction <- test$cement * test$super_plast
test$sqrt_age <- sqrt(test$age)
test$transformed_cement <- sqrt(test$cement)
# Ensure the log_strength and cement_superplast_composite columns are not present
test <- test[, !names(test) %in% c("log_strength", "cement_superplast_composite")]
# Confirm that the structure of the test data is correct
str(test)
# Generate predictions from the Box-Cox transformed linear regression model
test_predictions_boxcox <- predict(model_boxcox_refined_v3, newdata = test)
# Refine the model further by removing sqrt_age
model_boxcox_refined_v3_no_log <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, data = train_fold)
# Check VIF again
vif_values_v3 <- vif(model_boxcox_refined_v3)
print(vif_values_v3)
# Refine the model further by removing sqrt_age
model_boxcox_refined_v3_no_log <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, data = train_fold)
# Check VIF again
vif_values_v3 <- vif(model_boxcox_refined_v3_no_log)
print(vif_values_v3)
# Rebuild the model excluding log_strength for predictions
model_test_refined <- lm(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, data = train_fold)
# Prepare data matrix for glmnet (Ridge requires matrix input)
X <- model.matrix(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, train_fold)[, -1]
y <- train_fold$strength
# Ridge Regression with Cross-Validation
set.seed(123)
ridge_cv <- cv.glmnet(X, y, alpha = 0)
ridge_best_lambda <- ridge_cv$lambda.min
ridge_model <- glmnet(X, y, alpha = 0, lambda = ridge_best_lambda)
# Prepare validation set matrix (ensure the same transformations are applied)
X_val <- model.matrix(strength ~ cement + slag + flyash + water + super_plast + coarse_agg + fine_agg + age + cement_superplast_interaction, val_fold)[, -1]
# Make predictions using Ridge model
ridge_predictions <- predict(ridge_model, newx = X_val, s = ridge_best_lambda)
# Evaluate predictions
ridge_mae <- mean(abs(ridge_predictions - val_fold$strength))
ridge_r2 <- 1 - sum((val_fold$strength - ridge_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
# Print results
cat("Ridge Regression MAE:", ridge_mae, "\n")
cat("Ridge Regression R-squared:", ridge_r2, "\n")
# Lasso Regression with Cross-Validation
set.seed(123)
lasso_cv <- cv.glmnet(X, y, alpha = 1)
lasso_best_lambda <- lasso_cv$lambda.min
lasso_model <- glmnet(X, y, alpha = 1, lambda = lasso_best_lambda)
# Make predictions using Lasso model
lasso_predictions <- predict(lasso_model, newx = X_val, s = lasso_best_lambda)
# Evaluate predictions
lasso_mae <- mean(abs(lasso_predictions - val_fold$strength))
lasso_r2 <- 1 - sum((val_fold$strength - lasso_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
# Print results
cat("Lasso Regression MAE:", lasso_mae, "\n")
cat("Lasso Regression R-squared:", lasso_r2, "\n")
# Include polynomial features for selected variables
poly_features <- poly(train_fold$cement, degree = 2, raw = TRUE)
train_fold$cement_poly1 <- poly_features[, 1]
train_fold$cement_poly2 <- poly_features[, 2]
# Apply the same transformations to validation set
val_poly_features <- poly(val_fold$cement, degree = 2, raw = TRUE)
val_fold$cement_poly1 <- val_poly_features[, 1]
val_fold$cement_poly2 <- val_poly_features[, 2]
# Build a model with polynomial features
model_poly <- lm(strength ~ cement_poly1 + cement_poly2 + slag + flyash + water + super_plast + coarse_agg + fine_agg + age, data = train_fold)
poly_predictions <- predict(model_poly, newdata = val_fold)
# Evaluate the polynomial model
poly_mae <- mean(abs(poly_predictions - val_fold$strength))
poly_r2 <- 1 - sum((val_fold$strength - poly_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Polynomial Regression MAE:", poly_mae, "\n")
cat("Polynomial Regression R-squared:", poly_r2, "\n")
library(randomForest)
# Remove 'log_strength', 'cement_superplast_composite', and any polynomial features from the training data
train_fold <- train_fold[, !grepl("log_strength|cement_superplast_composite|poly", names(train_fold))]
# Retrain the Random Forest model without unnecessary features
rf_model <- randomForest(strength ~ ., data = train_fold, ntree = 500)
# Evaluate the Random Forest model on the validation data
rf_predictions <- predict(rf_model, newdata = val_fold)
# Calculate MAE and R2 for validation data
rf_mae <- mean(abs(rf_predictions - val_fold$strength))
rf_r2 <- 1 - sum((val_fold$strength - rf_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Random Forest MAE:", rf_mae, "\n")
cat("Random Forest R-squared:", rf_r2, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
# Define the grid of hyperparameters to tune
# Define the hyperparameter grid
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5, 6, 7, 8))
# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
strength ~ .,
data = train_fold,
method = "rf",
trControl = train_control,
tuneGrid = tune_grid
)
# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)
# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
# Calculate the residuals for the validation set using the tuned Random Forest model
residuals_rf_tuned <- val_fold$strength - rf_tuned_predictions
# 1. Linearity Check
plot(rf_tuned_predictions, val_fold$strength,
xlab = "Predicted Strength",
ylab = "Actual Strength",
main = "Linearity Check - Tuned Random Forest")
abline(0, 1, col = "blue")  # Line of perfect prediction
# 2. Homoscedasticity Check
plot(rf_tuned_predictions, residuals_rf_tuned,
xlab = "Predicted Strength",
ylab = "Residuals",
main = "Homoscedasticity and Residuals Check - Tuned Random Forest")
abline(h = 0, col = "red")  # Reference line for residuals at zero
# 3. Normality of Residuals
hist(residuals_rf_tuned,
main = "Histogram of Residuals - Tuned Random Forest",
xlab = "Residuals")
qqnorm(residuals_rf_tuned)
qqline(residuals_rf_tuned, col = "red")  # Reference line for normality
# 4. Independence of Residuals
plot(1:length(residuals_rf_tuned), residuals_rf_tuned,
xlab = "Observation Number",
ylab = "Residuals",
main = "Independence of Residuals Check - Tuned Random Forest")
abline(h = 0, col = "red")  # Reference line at zero
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
# Define the grid of hyperparameters to tune (only mtry for Random Forest)
tune_grid <- expand.grid(mtry = c(2, 4, 6, 8))
# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
strength ~ .,
data = train_fold,
method = "rf",
trControl = train_control,
tuneGrid = tune_grid,
ntree = 500,  # Set fixed ntree value here
importance = TRUE
)
# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)
# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
str(test)
test <- read.csv("data/data-test.csv")
str(test)
# Apply necessary transformations to the test data, ensuring no 'log_strength', 'cement_superplast_composite', or polynomial features
test$cement_superplast_interaction <- test$cement * test$super_plast
test$sqrt_age <- sqrt(test$age)
test$transformed_cement <- sqrt(test$cement)
# Ensure the log_strength and cement_superplast_composite columns are not present
test <- test[, !names(test) %in% c("log_strength", "cement_superplast_composite")]
# Confirm that the structure of the test data is correct
str(test)
# Generate predictions from the Box-Cox transformed linear regression model
test_predictions_boxcox <- predict(model_boxcox_refined_v3, newdata = test)
# Generate predictions from the Box-Cox transformed linear regression model
test_predictions_boxcox <- predict(model_boxcox_refined_v3_no_log, newdata = test)
# Generate predictions from the tuned Random Forest model
test_predictions_rf <- predict(rf_tuned_model, newdata = test)
# Ensemble the predictions using a weighted average
# You can adjust the weights as needed (e.g., 0.5 for each model)
ensemble_weights <- c(0.5, 0.5)  # Equal weighting for both models
test_predictions_ensemble <- ensemble_weights[1] * test_predictions_boxcox +
ensemble_weights[2] * test_predictions_rf
# Ensure the length of predictions matches the number of rows in the test dataset
print(length(test_predictions_ensemble))  # Should be 205
print(nrow(test))  # Should be 205
# Check if the id column exists
if ("id" %in% colnames(test)) {
print("id column is present")
} else {
print("id column is missing")
}
# Store the predictions in a data frame
submission_df <- data.frame(id = test$id, strength = test_predictions)
# Print the predictions to review them
print(submission_df)
# Store the ensemble predictions in a data frame
submission_df <- data.frame(id = test$id, strength = test_predictions_ensemble)
# Print the ensemble predictions to review them
print(submission_df)
# Store the ensemble predictions in a data frame
submission_df <- data.frame(id = test$id, strength = test_predictions_ensemble)
# Print the ensemble predictions to review them
print(submission_df)
# Save the ensemble predictions to a CSV file for submission
write.csv(submission_df, file = "test_predictions_ensemble_submission.csv", row.names = FALSE)
# Hyperparameter Tuning for Random Forest (keeping the existing approach)
set.seed(123)
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)
# Define the hyperparameter grid
tune_grid_rf <- expand.grid(mtry = c(2, 4, 6, 8), ntree = c(300, 500, 700))
# Train the Random Forest model with hyperparameter tuning
rf_tuned_model <- train(
strength ~ .,
data = train_fold,
method = "rf",
trControl = train_control,
tuneGrid = tune_grid_rf,
importance = TRUE
)
# Define the train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
# Define the grid of hyperparameters to tune (only mtry for Random Forest)
tune_grid <- expand.grid(mtry = c(2, 4, 6, 8))
# Train the Random Forest model using caret with hyperparameter tuning
rf_tuned_model <- train(
strength ~ .,
data = train_fold,
method = "rf",
trControl = train_control,
tuneGrid = tune_grid,
ntree = 500,  # Set fixed ntree value here
importance = TRUE
)
# Evaluate the best tuned model on the validation data
rf_tuned_predictions <- predict(rf_tuned_model, newdata = val_fold)
# Calculate MAE and R2 for the tuned model
rf_tuned_mae <- mean(abs(rf_tuned_predictions - val_fold$strength))
rf_tuned_r2 <- 1 - sum((val_fold$strength - rf_tuned_predictions)^2) / sum((val_fold$strength - mean(val_fold$strength))^2)
cat("Tuned Random Forest MAE:", rf_tuned_mae, "\n")
cat("Tuned Random Forest R-squared:", rf_tuned_r2, "\n")
test <- read.csv("data/data-test.csv")
str(test)
# Apply necessary transformations to the test data, ensuring no 'log_strength', 'cement_superplast_composite', or polynomial features
test$cement_superplast_interaction <- test$cement * test$super_plast
test$sqrt_age <- sqrt(test$age)
test$transformed_cement <- sqrt(test$cement)
# Ensure the log_strength and cement_superplast_composite columns are not present
test <- test[, !names(test) %in% c("log_strength", "cement_superplast_composite")]
# Confirm that the structure of the test data is correct
str(test)
# Generate predictions from the Box-Cox transformed linear regression model
test_predictions_boxcox <- predict(model_boxcox_refined_v3_no_log, newdata = test)
# Generate predictions from the tuned Random Forest model
test_predictions_rf <- predict(rf_tuned_model, newdata = test)
# Ensemble the predictions using a weighted average
# You can adjust the weights as needed (e.g., 0.5 for each model)
ensemble_weights <- c(0.5, 0.5)  # Equal weighting for both models
test_predictions_ensemble <- ensemble_weights[1] * test_predictions_boxcox +
ensemble_weights[2] * test_predictions_rf
# Ensure the length of predictions matches the number of rows in the test dataset
print(length(test_predictions_ensemble))  # Should be 205
print(nrow(test))  # Should be 205
# Check if the id column exists
if ("id" %in% colnames(test)) {
print("id column is present")
} else {
print("id column is missing")
}
# Store the ensemble predictions in a data frame
submission_df <- data.frame(id = test$id, strength = test_predictions_ensemble)
# Print the ensemble predictions to review them
print(submission_df)
summary(test_predictions_ensemble)
# Interpret the Box-Cox Transformed Linear Regression Model
summary(model_boxcox_refined_v3_no_log)
# Interpret the Variable Importance from Random Forest
importance(rf_tuned_model$finalModel)
# Perform ANOVA for key ingredients (cement, slag, flyash, water, super_plast)
anova_result <- aov(strength ~ cement + slag + flyash + water + super_plast, data = train_filtered)
summary(anova_result)
# Perform T-test for age groups (<=28 days and >28 days)
young <- subset(train_filtered, age <= 28)
old <- subset(train_filtered, age > 28)
ttest_result <- t.test(young$strength, old$strength)
print(ttest_result)
# Find the combination with the maximum mean strength across key ingredients and age
mean_strength <- aggregate(strength ~ cement + slag + flyash + water + super_plast + age,
data = train_filtered, FUN = mean)
max_mean_strength <- mean_strength[which.max(mean_strength$strength), ]
print(max_mean_strength)
# Perform ANOVA for key ingredients (cement, slag, flyash, water, super_plast)
anova_result <- aov(strength ~ cement + slag + flyash + water + super_plast, data = train_filtered)
summary(anova_result)
# Perform T-test for age groups (<=28 days and >28 days)
young <- subset(train_filtered, age <= 28)
old <- subset(train_filtered, age > 28)
ttest_result <- t.test(young$strength, old$strength)
print(ttest_result)
# Find the combination with the maximum mean strength across key ingredients and age
mean_strength <- aggregate(strength ~ cement + slag + flyash + water + super_plast + age,
data = train_filtered, FUN = mean)
max_mean_strength <- mean_strength[which.max(mean_strength$strength), ]
print(max_mean_strength)
